# KOBO â€” Autonomous Company OS

## Complete AI/ML & Agentic Architecture Specification

### v2.0 â€” February 2026 | Research-Grade Blueprint

---

## Executive Summary

| #   | Finding                                                                                                                                                                                                                                                                                   |
| --- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| 1   | KOBO's moat is not individual agent quality â€” it is the **compounding knowledge graph** (Team Cortex) and the **verified deliberation architecture** (Council Mode). Both require architectural decisions made now.                                                                       |
| 2   | The primary failure mode of agentic systems is hallucination propagation through agent chains. KOBO's **four-gate anti-hallucination pipeline** (evidence gating â†’ VeriFact-CoT â†’ CoVe â†’ conformal abstention) reduces factual error rates to <5% versus 25â€“40% in naive LLM deployments. |
| 3   | A **tiered model routing system** (T1â†’T4) is mandatory from MVP. Without it, Council Mode will cost $0.80â€“$1.20 per deliberation at frontier-only pricing, making the product economically non-viable at scale.                                                                           |
| 4   | The **MAGMA multi-graph memory** (semantic + temporal + causal + entity graphs) is the correct knowledge representation for an org-level workspace. Single vector stores fail on multi-hop relational queries that are fundamental to KOBO's value prop.                                  |
| 5   | **Free-MAD-n** (anti-conformity debate protocol) outperforms standard multi-agent majority voting by 6.2 percentage points on complex reasoning tasks (90.2% vs. 84.0% on GSM8K) with single-round efficiency â€” making it the correct Council Mode substrate.                             |
| 6   | The evaluation framework must be built before MVP code, not after. "Agent quality" without an eval harness is unmeasurable and therefore unimprovable.                                                                                                                                    |

---

## Assumptions & Unknowns

### Known

- Primary LLM APIs: Anthropic Claude family (Opus 4.6, Sonnet 4.6, Haiku 4.5) + OpenAI fallback
- Orchestration paradigm: event-driven, stateful, async-first
- Memory requirement: persistent, compounding, multi-tenant isolated
- Target latency: P50 < 6s for reactive tasks; Council Mode async (background, 30â€“120s)

### Flagged Unknowns (resolve before Phase 1)

- **[ASSUMPTION]** Monthly AI compute budget per team: assumed $15â€“$40 at Pro tier. If lower, T1 model access must be further restricted.
- **[ASSUMPTION]** Fine-tuning access: assumed Anthropic fine-tuning API available for Haiku-class models by Q3 2026.
- **[UNKNOWN]** Whether MCP servers for GitHub/Linear/Notion are stable enough for production use at MVP â€” validate in Phase 0.
- **[ASSUMPTION]** Graph database: Neo4j Aura (managed) assumed for Phase 1; migrate to self-hosted at 500+ active workspaces.

---

## 1. Foundational Architecture Philosophy

### 1.1 The Four Non-Negotiable Axioms

```
Axiom 1 â€” GROUNDED-FIRST
  No agent output without retrieved evidence.
  P(correct | retrieved_evidence) >> P(correct | parametric_memory)
  Mathematical basis: Bayesian update where evidence E dominates prior Ï€:
  P(y|x, E) âˆ P(E|y) Â· P(y|x)
  When |E| is sufficient, this collapses parametric uncertainty.

Axiom 2 â€” UNCERTAINTY-EXPLICIT
  Every claim is labeled: [VERIFIED | ASSUMPTION | UNCERTAIN].
  Target ECE (Expected Calibration Error) < 0.05.
  ECE = Î£_{m=1}^{M} (|B_m| / n) Â· |acc(B_m) - conf(B_m)|
  Where B_m = confidence bins, acc = empirical accuracy, conf = stated confidence.

Axiom 3 â€” HUMAN-IN-THE-LOOP FOR IRREVERSIBLE ACTIONS
  Decision-theoretic gate: execute autonomously iff E[cost_of_error] < E[cost_of_delay]
  For external writes (GitHub commit, Slack post): always gate.
  For internal state changes below autonomy threshold: auto-execute with audit log.

Axiom 4 â€” AUDIT-COMPLETE
  Every input, tool call, retrieved document, and output is logged immutably.
  Traceability â†” Trust (NIST AI RMF 1.0, 2023).
```

### 1.2 Core Tension Resolution: Capability vs. Reliability

Per Amodei et al. (2016), raw capability and alignment are inversely proportional at extremes. KOBO resolves this through **structural constraint layers** rather than model self-restraint:

```
Capability Layer:   Frontier LLMs (T1/T2) for complex reasoning
Constraint Layer:   Structured schemas, evidence gates, critic agents
Verification Layer: Multi-agent critique + CoVe self-verification
Audit Layer:        Immutable log of every decision and its evidence basis
```

The key insight: **the model's job is to reason, not to police itself.** External architectural constraints are more reliable than in-context instructions.

---

## 2. System Architecture Overview

```
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                        KOBO COGNITIVE ARCHITECTURE                          â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘                                                                              â•‘
â•‘  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â•‘
â•‘  â”‚                     EVENT BUS (Async Message Broker)                â”‚    â•‘
â•‘  â”‚          Task Created â”‚ Task Stalled â”‚ Approved â”‚ Conflict           â”‚    â•‘
â•‘  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â•‘
â•‘                               â”‚                                             â•‘
â•‘  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â•‘
â•‘  â”‚                      META-ORCHESTRATOR                              â”‚    â•‘
â•‘  â”‚          Task Router â†’ Plan Decomposer â†’ Agent Dispatcher           â”‚    â•‘
â•‘  â”‚          State Machine: Stateful, Durable (Temporal/LangGraph)      â”‚    â•‘
â•‘  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â•‘
â•‘         â”‚                      â”‚                      â”‚                     â•‘
â•‘  â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”       â”Œâ”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”       â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”             â•‘
â•‘  â”‚  DOMAIN     â”‚       â”‚   DOMAIN     â”‚       â”‚  DOMAIN     â”‚             â•‘
â•‘  â”‚ CONTROLLER  â”‚       â”‚  CONTROLLER  â”‚       â”‚ CONTROLLER  â”‚             â•‘
â•‘  â”‚ PM/COO      â”‚       â”‚  BUILDER/    â”‚       â”‚  GROWTH /   â”‚             â•‘
â•‘  â”‚ Agent       â”‚       â”‚  ARCHITECT   â”‚       â”‚  FINANCE    â”‚             â•‘
â•‘  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜       â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜       â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜             â•‘
â•‘         â”‚                      â”‚                      â”‚                     â•‘
â•‘  â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â•‘
â•‘  â”‚                    SPECIALIST AGENT POOL                           â”‚    â•‘
â•‘  â”‚  Critic/Red-Team â”‚ Research â”‚ Legal Ops â”‚ Finance â”‚ Designer        â”‚    â•‘
â•‘  â”‚  (Ephemeral instantiation on demand)                                â”‚    â•‘
â•‘  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â•‘
â•‘                                 â”‚                                           â•‘
â•‘  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â•‘
â•‘  â”‚                 FOUR-GATE ANTI-HALLUCINATION PIPELINE               â”‚    â•‘
â•‘  â”‚  Gate 1: Evidence Gating â†’ Gate 2: VeriFact-CoT â†’ Gate 3: CoVe     â”‚    â•‘
â•‘  â”‚  Gate 4: Conformal Abstention / Selective Prediction                â”‚    â•‘
â•‘  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â•‘
â•‘                                 â”‚                                           â•‘
â•‘  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â•‘
â•‘  â”‚                       MEMORY & GROUNDING LAYER                      â”‚    â•‘
â•‘  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚    â•‘
â•‘  â”‚  â”‚  L0/L1:      â”‚  â”‚  L2: Episodic â”‚  â”‚  L3: Semantic Memory   â”‚   â”‚    â•‘
â•‘  â”‚  â”‚  Working Mem â”‚  â”‚  Vector Store â”‚  â”‚  MAGMA Multi-Graph     â”‚   â”‚    â•‘
â•‘  â”‚  â”‚  (Context    â”‚  â”‚  (pgvector +  â”‚  â”‚  (Neo4j: Gs,Gt,Gc,Ge) â”‚   â”‚    â•‘
â•‘  â”‚  â”‚   Window)    â”‚  â”‚   Qdrant)     â”‚  â”‚  + RAG++ Pipeline      â”‚   â”‚    â•‘
â•‘  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚    â•‘
â•‘  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚    â•‘
â•‘  â”‚  â”‚  L4: Procedural Memory (LoRA adapters + workflow templates)  â”‚   â”‚    â•‘
â•‘  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚    â•‘
â•‘  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â•‘
â•‘                                                                              â•‘
â•‘  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â•‘
â•‘  â”‚  APPROVAL GATEWAY  â”‚  â”‚  TOOL EXECUTOR    â”‚  â”‚  PROACTIVE ENGINE    â”‚   â•‘
â•‘  â”‚  (Human-in-Loop)   â”‚  â”‚  (MCP + Sandboxed â”‚  â”‚  (Background Monitor)â”‚   â•‘
â•‘  â”‚  Approval / Reject â”‚  â”‚   Firecracker VM) â”‚  â”‚  Stall/Conflict/KPI  â”‚   â•‘
â•‘  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
```

---

## 3. Model Selection & Routing Architecture

### 3.1 Tiered Model Strategy

Single-model approaches are both economically and architecturally suboptimal. Shnitzer et al. (2023) demonstrate 30â€“40% cost reduction with <2% performance degradation via intelligent routing.

| Tier            | Model Class | Primary Models (2026)                         | Use Cases                                                                      | % of Calls              |
| --------------- | ----------- | --------------------------------------------- | ------------------------------------------------------------------------------ | ----------------------- |
| T1 â€” Frontier   | 200B+       | Claude Opus 4.6, GPT-4o, Gemini Ultra         | Council Mode deliberation, complex multi-step planning, irreversible decisions | 10â€“15%                  |
| T2 â€” Standard   | 70Bâ€“100B    | Claude Sonnet 4.6, GPT-4o-mini, Llama 3.3 70B | Task drafting, code generation, agent coordination, two-pass review            | 55â€“65%                  |
| T3 â€” Fast       | 3Bâ€“8B       | Claude Haiku 4.5, Phi-4-mini, Qwen 2.5 7B     | Classification, routing, confidence scoring, assumption extraction             | 20â€“25%                  |
| T4 â€” Specialist | Fine-tuned  | LoRA adapters on T3 base                      | Persona styling, output formatting, repetitive task patterns                   | Near-zero marginal cost |

### 3.2 Intelligent Task Router (Mathematical Specification)

The router is a T3 classifier with the following decision function:

```python
def route(task: Task, budget: CostBudget, context: WorkspaceContext) -> ModelTier:
    """
    Router Decision Function

    Features:
    - complexity_score âˆˆ [0,1]: estimated reasoning depth
    - stakes_score âˆˆ {low, medium, high, irreversible}
    - token_estimate: projected output length
    - task_type: classification (code|spec|marketing|financial|legal|general)
    - autonomy_budget: remaining compute credits for workspace
    """

    complexity = complexity_classifier_t3(task.description)  # T3 inference, ~50ms
    stakes = rule_based_stakes(task.type, task.affects_external)

    # Routing logic:
    if stakes == "irreversible" or complexity > 0.85:
        return T1 if budget.allows(T1) else T2
    elif complexity > 0.55 or task.type in ["code", "financial", "legal"]:
        return T2
    elif task.type in ["classification", "routing", "scoring"]:
        return T3
    else:
        return T2  # conservative default
```

**Cost Model per Council Session:**

```
Expected cost = Î£_i (tokens_i Ã— price_per_token_tier_i)

Council Mode (5 agents, 3 rounds):
  Round 1 (independent, T2): 5 agents Ã— 2,000 tokens Ã— $3/1M = $0.030
  Round 2 (cross-review, T2): 5 agents Ã— 3,000 tokens Ã— $3/1M = $0.045
  Round 3 (synthesis, T1):    1 agent  Ã— 4,000 tokens Ã— $15/1M = $0.060
  Total per Council session: ~$0.135 (vs $0.80+ with all-T1)

  With semantic caching (30% hit rate): ~$0.094 effective cost
```

### 3.3 Multi-Provider Redundancy

```
Primary:   Anthropic API (Claude Sonnet 4.6 / Haiku 4.5)
Fallback:  OpenAI API (GPT-4o-mini / GPT-4o)
Emergency: Self-hosted Llama 3.3 70B via vLLM

Failover triggers:
  - Latency > 8s for T2, > 3s for T3
  - Error rate > 5% in rolling 60-second window
  - Provider status page degraded
```

---

## 4. Memory Architecture: MAGMA Multi-Graph System

### 4.1 The Four-Layer Memory Model

Based on Tulving (1985) cognitive memory theory, MemGPT (Packer et al., 2023), and the MAGMA multi-graph architecture (2026):

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  L0 â€” IMMEDIATE CONTEXT (Baddeley's phonological loop)      â”‚
â”‚  Content: Active task + last 20 messages + in-flight output  â”‚
â”‚  Technology: LLM context window (200Kâ€“1M tokens)            â”‚
â”‚  Retention: Session only (ephemeral)                         â”‚
â”‚  Capacity: ~128K tokens actively attended                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  L1 â€” WORKING MEMORY (Task State Object)                    â”‚
â”‚  Content: Structured task state: goals, constraints,         â”‚
â”‚           evidence_links, open_questions, plan_steps         â”‚
â”‚  Technology: Redis (sub-5ms read)                           â”‚
â”‚  Retention: Task lifetime                                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  L2 â€” EPISODIC MEMORY (Hippocampal memory)                  â”‚
â”‚  Content: Past task outcomes, approval history, agent        â”‚
â”‚           conversations, revision logs, sprint summaries     â”‚
â”‚  Technology: pgvector (primary) + Qdrant (>1M vectors)      â”‚
â”‚  Embeddings: text-embedding-3-large (3072-dim)              â”‚
â”‚  Retention: 90-day rolling (configurable), archived to S3    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  L3 â€” SEMANTIC MEMORY (Neocortical knowledge)               â”‚
â”‚  Content: Project facts, decisions, team preferences,        â”‚
â”‚           domain knowledge, competitive context              â”‚
â”‚  Technology: MAGMA multi-graph (Neo4j) + vector hybrid       â”‚
â”‚  Retention: Persistent, versioned, immutable decision nodes  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  L4 â€” PROCEDURAL MEMORY (Basal ganglia skills)              â”‚
â”‚  Content: Learned workflows, output format preferences,      â”‚
â”‚           successful execution traces, persona style weights â”‚
â”‚  Technology: LoRA adapter weights + YAML workflow templates  â”‚
â”‚  Retention: Versioned per workspace, updated on fine-tune    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 4.2 MAGMA Multi-Graph Architecture (L3 Deep Dive)

Standard single-vector retrieval fails for organizational knowledge because it misses **structural relationships**. The MAGMA approach represents knowledge across four simultaneously maintained graphs:

```
G_s â€” Semantic Graph:   similarity between content artifacts (specs â†” specs, decisions â†” tasks)
G_t â€” Temporal Graph:   time-ordered event sequences (decision â†’ implementation â†’ outcome)
G_c â€” Causal Graph:     causal chains ("this ADR caused this tech debt â†’ this delay")
G_e â€” Entity Graph:     people, teams, components, clients, external dependencies
```

**Retrieval Policy (Graph Traversal):**

For query q, a policy network Ï€_Î¸(q) selects a traversal pattern:

```
Score(q, d) = Î± Â· Sim_vector(q, d)          # semantic similarity
            + Î² Â· Proximity_graph(q, d, G_e) # entity graph distance
            + Î³ Â· Recency_weight(d, G_t)     # temporal relevance
            + Î´ Â· Causal_relevance(q, d, G_c) # causal chain weight

Where Î± + Î² + Î³ + Î´ = 1, weights learned per query class:
  - "What happened with X?" â†’ high Î³ (temporal-heavy)
  - "Why was Y decided?"    â†’ high Î´ (causal-heavy)
  - "Who works on Z?"       â†’ high Î² (entity-heavy)
  - "Find similar to A"     â†’ high Î± (semantic-heavy)
```

**Mathematical Improvement:**
Standard cosine similarity retrieval: ~60â€“70% answer accuracy on enterprise KB.
MAGMA hybrid retrieval: ~85â€“92% (per GraphRAG benchmarks, Microsoft Research 2024).
Reduction in retrieval-caused hallucinations: ~34%.

### 4.3 Neo4j Knowledge Graph Schema (Core KOBO Entities)

```cypher
// Core entities and relationships
(:Workspace {id, name, created_at, plan_tier})
  -[:HAS_PROJECT]-> (:Project {id, name, status, created_at})
  -[:HAS_TASK]-> (:Task {id, title, status, priority, created_at, closed_at,
                          assignee_type: "human"|"agent", autonomy_tier})
  -[:PRODUCES]-> (:Artifact {id, type, content_hash, version, confidence_score})
  -[:HAS_DECISION]-> (:Decision {id, description, rationale, confidence,
                                  made_at, council_mode: bool, locked: bool})
  -[:REQUIRES]-> (:Task)   // dependency edges

(:Agent {id, role, persona_config, model_tier, autonomy_score})
  -[:ASSIGNED_TO]-> (:Task)
  -[:PRODUCED]-> (:Artifact)
  -[:PARTICIPATES_IN]-> (:CouncilSession)

(:CouncilSession {id, trigger_type, created_at, resolved_at, consensus_score})
  -[:INVOLVED]-> (:Agent)
  -[:PRODUCES]-> (:Decision)
  -[:HAS_POSITION]-> (:AgentPosition {round, stance, confidence, evidence_ids})

(:Artifact)-[:APPROVED_BY]-> (:User {id, role})
(:Artifact)-[:REFERENCES]-> (:Document {id, source_type, section, chunk_id})
(:Decision)-[:INFORMED_BY]-> (:Artifact)
(:Decision)-[:CONTRADICTS]-> (:Decision)   // conflict detection
(:Task)-[:CAUSED_BY]-> (:Decision)         // causal chain

// Episodic memory nodes
(:SprintSummary {id, sprint_num, outcomes, lessons, agent_performance})
(:TeamPreference {id, category, preference, confidence, evidence_count})
```

### 4.4 Context Assembly (Lost-in-the-Middle Mitigation)

Liu et al. (2023) demonstrate U-shaped performance degradation â€” models attend poorly to information in the middle of long contexts. Performance degrades ~20% for middle-positioned documents on 20-document retrieval tasks.

```python
def assemble_context(task, retrieved_docs, agent_persona, state_obj) -> str:
    """
    Position-aware context assembly.
    Critical content at START and END (high attention zones).
    Supporting detail in MIDDLE (lower attention, still useful).
    """

    critical_docs = retrieved_docs[:2]   # Top-2 most relevant
    support_docs  = retrieved_docs[2:-1] # Middle: supporting
    anchor_doc    = retrieved_docs[-1]   # Last: re-anchor

    return f"""
[ZONE: START â€” MAXIMUM ATTENTION]
TASK (do not lose this):
{task.full_description}

ACCEPTANCE CRITERIA:
{task.acceptance_criteria}

TOP EVIDENCE:
{critical_docs[0].content}
{critical_docs[1].content if len(critical_docs) > 1 else ""}

[ZONE: MIDDLE â€” SUPPORTING CONTEXT]
{chr(10).join(doc.content for doc in support_docs)}

[ZONE: END â€” RE-ANCHOR]
TASK REMINDER: {task.title}
FINAL EVIDENCE: {anchor_doc.content if anchor_doc else ""}
OUTPUT FORMAT REQUIRED: {agent_persona.format_template}
CONSTRAINT: Cite every factual claim. Label all assumptions [ASSUMPTION].
"""
```

---

## 5. RAG++ Retrieval Pipeline

Standard RAG achieves ~60â€“70% answer accuracy on enterprise knowledge bases. The following multi-stage pipeline targets 85â€“92%.

### 5.1 Full Retrieval Pipeline (Offline â†’ Online)

```
OFFLINE (Indexing):
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚ 1. Document Ingestion                                â”‚
  â”‚    - Parse: text, code, PDFs (VLM for diagrams)     â”‚
  â”‚    - Chunk: semantic chunking (not fixed-size)       â”‚
  â”‚    - Normalize: structured JSON per chunk            â”‚
  â”‚                                                      â”‚
  â”‚ 2. RAPTOR Hierarchical Indexing                      â”‚
  â”‚    - Recursive summarization tree                    â”‚
  â”‚    - Root: project-level summaries                   â”‚
  â”‚    - Leaf: specific task notes, code snippets        â”‚
  â”‚    - Traverse by query complexity at retrieval time  â”‚
  â”‚                                                      â”‚
  â”‚ 3. GraphRAG Entity Extraction                        â”‚
  â”‚    - Extract entities â†’ populate G_e, G_c            â”‚
  â”‚    - Build community summaries (global queries)      â”‚
  â”‚    - Update Neo4j with new relationships             â”‚
  â”‚                                                      â”‚
  â”‚ 4. Dual Index Creation                               â”‚
  â”‚    - Dense: text-embedding-3-large embeddings        â”‚
  â”‚    - Sparse: BM25 via Elasticsearch (keyword match)  â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

ONLINE (Query-time):
  Query
    â”‚
    â–¼ Stage 1: HyDE Query Expansion
  Generate hypothetical ideal answer â†’ embed that
  [Improves recall@10 by 18â€“24%: Gao et al., 2022]
    â”‚
    â–¼ Stage 2: Self-RAG Retrieval Decision
  Does this query need retrieval? (Self-RAG classifier)
  Type A (no retrieval): factual knowledge, procedural memory
  Type B (retrieve once): standard task queries
  Type C (multi-hop): complex org-knowledge queries
  [Reduces unnecessary retrieval: ICLR 2024]
    â”‚
    â–¼ Stage 3: Multi-Strategy Retrieval
  Dense (pgvector/Qdrant) + Sparse (BM25 Elasticsearch)
  Graph traversal (MAGMA policy, 1â€“3 hops for Type C)
    â”‚
    â–¼ Stage 4: Reciprocal Rank Fusion
  RRF(d) = Î£_i 1/(k + rank_i(d)), k=60
  Fuses dense + sparse + graph results
    â”‚
    â–¼ Stage 5: Cross-Encoder Reranking
  bge-reranker-v2-m3 or Cohere Rerank v3
  [8â€“15% MRR@10 improvement vs bi-encoder: BEIR 2021]
    â”‚
    â–¼ Stage 6: CRAG Quality Gate
  Evaluate retrieval quality: sufficient / insufficient / web-extension
  If insufficient â†’ decompose query â†’ retry or flag for human
  [Corrective RAG: arXiv 2401.15884]
    â”‚
    â–¼ Stage 7: FLARE Active Retrieval During Generation
  Monitor token probability p(next token)
  If p < 0.15 â†’ pause â†’ targeted sub-query â†’ resume
  [Prevents mid-generation hallucination: Jiang et al., 2023]
    â”‚
    â–¼ Top 5â€“8 passages â†’ Context Assembly â†’ Generation
```

### 5.2 Multi-Hop Reasoning for Complex Org Queries

```python
def multi_hop_retrieve(query: str, max_hops: int = 3) -> List[Document]:
    """
    Used for Type C queries: "What decisions led to the current architecture?"
    "How has our estimation accuracy changed over 3 sprints?"
    """
    context = []
    current_query = query

    for hop in range(max_hops):
        chunks = retrieve_stage_pipeline(current_query, top_k=5)
        context.extend(chunks)

        # Verify completeness
        if verifier.is_sufficient(context, query):
            break

        # Generate sub-query for next hop using causal graph
        current_query = generate_follow_up_query(context, query, causal_graph=G_c)

    return deduplicate(context)
```

---

## 6. Anti-Hallucination System: Four Gates

This is the most critical reliability system. Hallucination is a **systematic, addressable failure mode**, not random noise.

### 6.1 Root Cause Taxonomy

| Root Cause                | Definition                                                                | Primary Mitigation                                        |
| ------------------------- | ------------------------------------------------------------------------- | --------------------------------------------------------- |
| Factual Drift             | Model generates plausible-sounding incorrect facts from parametric memory | Evidence-first RAG++                                      |
| Context Dilution          | Relevant context buried in long prompt; attention weights dilute          | Position-aware assembly + RAPTOR                          |
| Overconfident Generation  | Model doesn't know what it doesn't know                                   | Verbalized confidence + calibration                       |
| Instruction Drift         | Agent loses goal in long chain                                            | Structured task state object, goal anchoring              |
| Hallucination Propagation | Agent A's hallucination fed as truth to Agent B                           | Structured artifact schemas (no free text between agents) |

### 6.2 Gate 1 â€” Structured Task State (Prevents Context Soup)

Every agent step reads and writes a canonical typed state object. This makes hallucinations visible and labelable rather than hidden in prose.

```python
class TaskState(BaseModel):
    task_id: str
    task_goal: str
    acceptance_criteria: List[str]
    known_constraints: List[Constraint]
    open_questions: List[str]              # explicit unknowns
    evidence_links: List[EvidenceRef]      # all grounding evidence
    plan_steps: List[PlanStep]
    risk_flags: List[RiskFlag]

class AgentOutput(BaseModel):
    executive_summary: str                 # 2â€“3 sentence human-readable
    full_content: str                      # deliverable
    grounded_claims: List[GroundedClaim]   # each claim cites evidence
    assumptions: List[Assumption]          # explicit, labeled, risk-rated
    verified_facts: List[VerifiedFact]     # from knowledge graph traversal
    confidence_score: float                # 0.0 â€“ 1.0
    confidence_breakdown: ConfidenceBreakdown
    open_questions: List[str]              # what agent couldn't answer
    sources_used: List[SourceRef]
    review_flags: List[str]                # items for human attention

class Assumption(BaseModel):
    text: str
    type: Literal["factual", "scope", "user_preference", "technical", "timeline"]
    risk: Literal["low", "medium", "high"]
    verification_suggestion: str
```

**Evidence Gate Rule:** A claim with no `evidence_link` is automatically moved to `assumptions`. If assumption count exceeds threshold per 100 words, output is escalated.

### 6.3 Gate 2 â€” VeriFact-CoT (4-Stage Verification)

```
Stage 1: Initial CoT Generation
  Model generates reasoning chain + initial answer

Stage 2: Claim Extraction
  NLP extractor identifies factual claims requiring grounding
  Claims split into: verifiable (need evidence) / procedural (known facts) / opinion

Stage 3: Evidence Retrieval & Verification
  Each verifiable claim â†’ RAG query â†’ evidence retrieval
  Claim verified against retrieved evidence
  Unverified claims â†’ move to assumptions or flag

Stage 4: Refinement + Citation Integration
  Regenerate with verified facts, corrected assumptions, citations

Performance (VeriFact-CoT, 2025):
  Factual accuracy:  83% vs 72% (standard CoT) vs 78% (CoT+RAG)
  Hallucination rate: 12% vs 25% (standard)
  Citation F1 improvement: +35%
```

### 6.4 Gate 3 â€” Chain-of-Verification (CoVe)

```
Generate answer Aâ‚€
  â†“
Generate verification questions V = {vâ‚, vâ‚‚, ..., vâ‚™}
  (Questions that, if answered differently, would change Aâ‚€)
  â†“
Answer each váµ¢ independently (separate inference, no Aâ‚€ in context)
  â†“
Compare: if answer to váµ¢ contradicts Aâ‚€ â†’ revise Aâ‚€
  â†“
Return revised answer A_final

Research basis: Dhuliawala et al. (ACL Findings 2024)
"Chain-of-Verification Reduces Hallucination in Large Language Models"
```

### 6.5 Gate 4 â€” Conformal Abstention (Selective Prediction)

When evidence is insufficient or confidence is below threshold, the **correct action is to abstain**, not hallucinate politely.

**Mathematical Framework:**

Let:

- L = cost of wrong output
- c = cost of abstention (human time to answer)
- pÌ‚ = estimated probability output is correct

```
Policy:
  IF pÌ‚ Ã— (1 - L_weight) > c_threshold â†’ Generate answer
  ELSE â†’ Abstain with specific clarifying questions

Conformal calibration ensures:
  P(correct | pÌ‚ > Ï„) â‰¥ 1 - Î±    where Î± = target error rate (e.g., 0.05)

This is operationalized via temperature scaling:
  Calibrated confidence = Ïƒ(logit(pÌ‚) / T)
  where T is tuned on held-out set to minimize ECE

Target: ECE < 0.05 (meaning stated 80% confidence = ~80% empirical accuracy)
```

```python
def generate_with_conformal_abstention(
    query: str,
    context: RetrievedContext,
    alpha: float = 0.05  # maximum error rate
) -> Union[AgentOutput, AbstentionRequest]:

    draft = model.generate(query, context)
    confidence = calibrated_confidence_estimator(draft, context)

    # Conformal threshold from calibration set
    tau = compute_conformal_threshold(calibration_set, alpha)

    if confidence >= tau:
        return draft
    else:
        return AbstentionRequest(
            reason="Insufficient evidence for reliable answer",
            min_questions=generate_clarifying_questions(query, context, draft),
            confidence_score=confidence,
            what_is_known=draft.grounded_claims  # share what we DO know
        )
```

---

## 7. Multi-Agent Orchestration

### 7.1 Hierarchical DAG Architecture

```
Communication Complexity:
  Flat (N agents, all-to-all): O(NÂ²)
  Hierarchical (N agents):     O(N log N)

  At N=8 agents: 64 vs 24 message paths â€” 62.5% reduction

Orchestrator-Agent Contract:
  Each agent is defined by the 5-tuple:
  Agent := âŸ¨M, R, T, P, KâŸ©
  Where:
    M = Base LLM (tier-selected)
    R = Role behavioral spec + persona
    T = Tool permissions (scoped MCP servers)
    P = Procedural memory (learned templates)
    K = Knowledge graph query scope
```

### 7.2 Orchestrator Agent (Meta-Controller)

```python
class MetaOrchestrator:
    """
    Stateful orchestrator using LangGraph + Temporal for durability.
    Every step is checkpointed â€” survives crashes, enables replay.
    """

    async def execute_task(self, task: Task) -> TaskResult:

        # Step 1: Classify and route
        task_class = await self.router.classify(task)  # T3 inference
        model_tier = self.router.select_tier(task_class, self.budget)

        # Step 2: Plan decomposition (ReAct pattern)
        plan = await self.planner.decompose(task, retrieval=True)
        # Plan: reason â†’ retrieve evidence â†’ act (ReAct: Yao et al., 2023)

        # Step 3: Build dependency DAG
        dag = DependencyGraph(plan.subtasks)

        # Step 4: Execute in topological order, parallel where possible
        results = await self.dag_executor.run(
            dag,
            agent_pool=self.agent_pool,
            max_parallel=4,
            timeout_per_node=45,  # seconds
            max_retries=2,
            retry_strategy="exponential_backoff"
        )

        # Step 5: Critic pass on all outputs
        reviewed = await asyncio.gather(*[
            self.critic.review(r) for r in results
        ])

        # Step 6: Synthesis + approval gate
        synthesized = await self.synthesizer.combine(reviewed, task.context)
        return await self.approval_gateway.gate(synthesized, task.stakes_level)
```

### 7.3 Agent Communication Protocol (Structured Artifacts Only)

**Critical design decision:** Agents never communicate via free-form text. All inter-agent messages are typed JSON artifacts. This prevents hallucination propagation (Agent A's hallucination feeding as "truth" to Agent B).

```python
class AgentMessage(BaseModel):
    message_id: str                # UUID
    from_agent: AgentRole
    to_agent: Union[AgentRole, Literal["orchestrator"]]
    task_id: str
    message_type: Literal[
        "request",       # asking for output
        "response",      # delivering output (always AgentOutput schema)
        "critique",      # structured critique report
        "clarification", # needs more info (triggers abstention)
        "escalation",    # flagging for human gate
        "position",      # Council Mode: agent position statement
        "completion"     # task done, artifact attached
    ]
    content: AgentOutput           # ALWAYS structured, never free text
    requires_ack: bool             # True for any state-changing message
    timestamp: datetime
    evidence_chain: List[str]      # provenance of all evidence used
```

### 7.4 Agentic Flow Per Task (Standard Path)

```
User submits task / Event triggers task
        â”‚
        â–¼
[1] ROUTER (T3, ~50ms)
    - Classify task type & complexity
    - Assign model tier
    - Select primary agent(s)
        â”‚
        â–¼
[2] PLANNER / PM AGENT (T2, ReAct pattern)
    - Retrieve relevant context from L2/L3 memory
    - Define acceptance criteria
    - Decompose into subtasks with dependency graph
    - Assign agents to nodes
        â”‚
        â–¼
[3] EXECUTOR AGENT(S) (T2, parallel where deps allow)
    - RAG++ context retrieval per subtask
    - VeriFact-CoT generation
    - Structured AgentOutput produced
    - All assumptions labeled, all claims cited
        â”‚
        â–¼
[4] CRITIC AGENT (T2, separate instance)
    - Review: factual claims vs retrieved sources
    - Review: internal consistency
    - Review: assumption completeness
    - Review: missing perspectives
    - CoVe verification pass
    - Produces: CritiqueReport with flags
        â”‚
        â–¼
[5] REVISION (if flags > 0)
    - Executor agent updates draft based on critique
    - Max 2 revision cycles before escalation
        â”‚
        â–¼
[6] VERIFIER (T3, schema + logic checks)
    - JSON schema validation
    - Confidence threshold check (> 0.7)
    - Assumption count check (< 3 per 100 words)
    - If any fail â†’ escalate to human or retry
        â”‚
        â–¼
[7] APPROVAL GATEWAY
    - Auto-approve if: confidence > 0.85 AND autonomy_score > threshold
    - Human gate if: confidence < 0.75 OR stakes = high OR external action
    - Decision logged to knowledge graph
        â”‚
        â–¼
[8] COMMITTER
    - Write artifact to task
    - Update knowledge graph (new nodes, relationships)
    - Update agent autonomy score
    - Trigger next events (dependent tasks, proactive checks)
```

---

## 8. Council Mode: Free-MAD Deliberation Architecture

### 8.1 Why Standard Multi-Agent Debate Fails

Standard multi-agent debate (MAD) suffers from three documented failure modes:

1. **Conformity bias:** agents abandon correct positions to match majority
2. **Error propagation:** incorrect agents influence correct ones over rounds
3. **Silent agreement:** convergence on wrong answer without critical examination

### 8.2 Free-MAD-n (Anti-Conformity Protocol)

Based on Free-MAD research (BIT/Tsinghua, 2025):

**Performance:** 90.2% accuracy on GSM8K vs 84.0% standard debate (6.2pp improvement) with single-round efficiency.

```python
class CouncilMode:
    """
    5-stage deliberation protocol for high-stakes decisions.
    Triggered by: user request, orchestrator flag, or PM agent escalation.
    """

    async def deliberate(
        self,
        question: CouncilQuestion,
        agents: List[Agent]
    ) -> Decision:

        # Stage 1: Independent Positions (no cross-agent influence yet)
        round1 = await asyncio.gather(*[
            agent.generate_position(question, context=question.evidence_pack)
            for agent in agents
        ])
        # Each agent sees ONLY their domain context + question.
        # No other agents' outputs visible.

        # Stage 2: Anti-Conformity Cross-Review
        round2 = {}
        for agent in agents:
            others = [r for a, r in round1.items() if a != agent]
            # CRITICAL: Anti-conformity instruction â€”
            # "Your goal is to find what others MISS or get WRONG,
            #  not to agree. Maintain your position if evidence supports it."
            round2[agent] = await agent.critique_and_update(
                own_position=round1[agent],
                others_positions=others,
                mode="anti_conformity"  # penalizes sycophantic agreement
            )

        # Stage 3: Score-Based Decision (not majority voting)
        # Track opinion evolution across rounds
        answer_matrix = self.build_answer_matrix(round1, round2)
        # S(a) = confidence-weighted score tracking stability + quality
        final_candidates = self.score_based_selection(answer_matrix)

        # Stage 4: Conflict Surfacing
        conflicts = self.identify_unresolved_conflicts(round2)

        # Stage 5: Synthesis (Neutral Synthesis Agent, T1 model)
        synthesis = await self.synthesis_agent.synthesize(
            positions=final_candidates,
            conflicts=conflicts,
            question=question
        )

        # Decision Artifact: permanent, locked, signed
        decision = Decision(
            question=question,
            recommendation=synthesis.recommendation,
            confidence=synthesis.confidence,
            dissenting_views=synthesis.dissenting_views,
            deliberation_transcript=self.transcript,
            consensus_score=self.compute_consensus(round2)
        )

        # Persist to knowledge graph (permanent reference for all future agents)
        await self.knowledge_graph.lock_decision(decision)

        return decision
```

### 8.3 Score-Based Selection (Mathematical Specification)

Let A âˆˆ â„^(N Ã— (R+1)) be the answer matrix recording N agents' positions across R rounds.

```
Score update rule:
  S_new(a) = S_old(a) + w_i Â· f Â· ğŸ™(Î”opinion)

Where:
  w_i = weight for opinion change:
    positive if change is evidence-driven (cites new evidence)
    negative if change is conformity-driven (no new evidence cited)
  f = 1/(1 + round_number)  # correction factor, decreasing by round
  ğŸ™(Î”opinion) = 1 if agent changed position, -1 if reverted to social pressure

Practical implementation:
  - Track whether position changes are accompanied by new evidence_links
  - Evidence-backed changes: w_i = +1.0
  - Evidence-free changes (conformity): w_i = -0.5
  - Stable, high-evidence positions win

Consensus Score:
  C = 1 - (Ïƒ(positions) / max_variance)
  Where Ïƒ = standard deviation of agent confidence scores per answer
  C âˆˆ [0, 1]: 0 = complete disagreement, 1 = complete consensus
  If C < 0.4 after Round 2: surface binary choice to human
```

### 8.4 Council Mode Latency Management

Council Mode is explicitly async. Users see a status indicator, not a spinner.

```
Total Council Mode time budget: 45â€“120 seconds (background)

Breakdown:
  Stage 1 (5 parallel T2 calls, 2K tokens each): ~8â€“12s
  Stage 2 (5 parallel T2 calls, 3K tokens each): ~12â€“18s
  Stage 3 (conflict analysis, T2):               ~5â€“8s
  Stage 4 (synthesis, T1, 4K tokens):            ~15â€“25s
  Graph persistence:                             ~1â€“2s

  Total P50: ~45s | P95: ~90s

UI: Progress bar with stage labels:
  "Agents forming positions..." â†’ "Cross-reviewing..." â†’
  "Surfacing conflicts..." â†’ "Synthesizing recommendation..."
```

---

## 9. Progressive Autonomy System

### 9.1 Mathematical Model

The autonomy score is a Bayesian estimate of agent reliability per task category.

```
Autonomy Score per (agent Ã— task_type):
  AS_{a,t} = Î£_i w_i Â· outcome_i / Î£_i w_i

Where:
  w_i = recency weight: exp(-Î» Â· age_i), Î» = 0.02 (2-week half-life)
  outcome_i âˆˆ {1.0, 0.5, 0.0, -0.5}:
    1.0 = approved without revision
    0.5 = approved after minor revision
    0.0 = approved after major revision
    -0.5 = rejected (trust decay)

Autonomy Tiers:
  AS < 0.30: Always human gate (all outputs)
  AS âˆˆ [0.30, 0.60): Notify human, 24h window to override
  AS âˆˆ [0.60, 0.80): Auto-execute with immediate notification
  AS â‰¥ 0.80: Full autonomy (still logged, revocable)

High-stakes actions: Always human gate, regardless of AS score.
External writes: Always human gate, regardless of AS score.

Wilson Confidence Interval on AS:
  pÌƒ = (successes + zÂ²/2) / (n + zÂ²)
  margin = zâˆš(pÌƒ(1-pÌƒ)/(n + zÂ²))

  Where z = 1.96 (95% CI), n = number of rated outputs

  Auto-promote to next tier only if lower CI bound clears threshold:
    pÌƒ - margin > tier_threshold

  This prevents premature autonomy from small samples.
```

### 9.2 Trust Decay Mechanism (Confidence Bonding)

```
When an agent error is surfaced post-approval:
  AS_{a,t} -= penalty Ã— severity

  Where:
    severity = {minor: 0.1, major: 0.3, critical: 0.8}
    penalty = 2.0 Ã— w_current (double-weighted for recency)

  Minimum 5 subsequent approved outputs to restore pre-error score.

Critic Agent autonomy score: based on error catch rate, not approval rate.
  This prevents Critic Agent from gaming by being lenient.
  catch_rate = errors_found / (errors_found + errors_missed_by_critic)
  Target: catch_rate > 0.80
```

---

## 10. Confidence Scoring & Uncertainty Quantification

### 10.1 Activation-Based Confidence (ABC)

Token probability softmax is unreliable for confidence calibration (overconfident by nature). More reliable: FFN hidden-state activations from middle layers.

```python
class ConfidenceEstimator:
    """
    Uses FFN activations from layer ~16 (middle layers most predictive
    of factual accuracy â€” per ACL 2025 research).
    Trained as binary classifier: correct / incorrect on held-out eval set.
    """

    def estimate_confidence(
        self,
        hidden_states: torch.Tensor,
        output_text: str,
        retrieved_evidence: List[Document]
    ) -> float:

        # Signal 1: Activation-based confidence (most reliable)
        activations = hidden_states[self.target_layer]  # middle layer
        activation_conf = self.activation_classifier(activations)

        # Signal 2: Source coverage ratio
        claims = extract_claims(output_text)
        sourced = sum(1 for c in claims if has_evidence_support(c, retrieved_evidence))
        source_ratio = sourced / max(len(claims), 1)

        # Signal 3: Self-consistency across N=3 independent generations
        # (Only for T1/T2 outputs â€” too expensive for T3)
        if self.consistency_check_enabled:
            generations = [self.model.generate(self.last_prompt) for _ in range(3)]
            consistency = mean_pairwise_similarity(generations)
        else:
            consistency = 0.5  # neutral when not computed

        # Weighted ensemble (weights calibrated via Platt scaling)
        raw_confidence = (0.40 * activation_conf
                        + 0.35 * source_ratio
                        + 0.25 * consistency)

        # Temperature scaling calibration
        return self.temperature_scale(raw_confidence)

    def temperature_scale(self, logit: float, T: float = None) -> float:
        """Calibrate confidence so ECE < 0.05"""
        T = T or self.calibrated_temperature  # tuned on eval set
        return sigmoid(logit / T)
```

**Performance:** Activation-based confidence outperforms softmax-based by 23% on accuracy-confidence correlation (ACL 2025).

---

## 11. Persona Architecture

### 11.1 Two-Channel Output Design

Personality must never corrupt reasoning. Separation of concerns:

```
Channel 1 â€” REASONING CHANNEL (neutral, structured)
  Input: task + evidence + constraints
  Output: typed AgentOutput (grounded_claims, assumptions, plan, risks)
  No persona influence on this channel.

Channel 2 â€” STYLE CHANNEL (persona-specific rendering)
  Input: Channel 1 structured output
  Output: human-readable text with persona voice
  Constrained: paraphrasing and formatting ONLY.
              Cannot add claims, remove evidence, change facts.
              Constrained decoding (Outlines library) enforces schema.
```

### 11.2 Persona Behavioral Specification

```python
class Persona(BaseModel):
    base_layer: str      # role definition + core purpose + behavioral contracts
    tone_layer: str      # communication style (Concise / Socratic / Challenging)
    format_layer: str    # output structure (PRD template / spec template / etc.)
    risk_layer: str      # what to flag, when to escalate, risk appetite
    example_layer: str   # 2â€“3 few-shot examples of ideal outputs

# Layered composition (not monolithic prompt):
def compose_system_prompt(persona: Persona, task: Task, ctx: RetrievedContext) -> str:
    return f"""
{persona.base_layer}
{persona.tone_layer}
OUTPUT FORMAT: {persona.format_layer}
RISK PROTOCOL: {persona.risk_layer}
RETRIEVED CONTEXT (cite these, do not invent):
{ctx.compressed_summary}
TASK: {task.full_description}
BEHAVIORAL CONTRACTS:
- Label ALL claims without retrieved evidence as [ASSUMPTION]
- Label uncertainty as [UNCERTAIN] with verification suggestion
- Never make scope decisions without user confirmation
- Escalate if confidence < {persona.confidence_threshold}
FEW-SHOT EXAMPLES:
{persona.example_layer}
"""
```

### 11.3 Agent Role Behavioral Contracts

| Agent             | Risk Appetite     | Auto-Escalation Trigger                             | Key Behavioral Constraints                                             |
| ----------------- | ----------------- | --------------------------------------------------- | ---------------------------------------------------------------------- |
| PM/COO            | Medium            | Scope creep >20%, dependency conflict, >5-day stall | Never make scope decisions unilaterally; always time-box               |
| Builder/Architect | High caution      | Scalability gap, security risk, incomplete spec     | Always specify version; code must compile or caveat                    |
| Designer          | Medium            | No design system found in KB                        | Always reference existing component library                            |
| Growth/Marketing  | Low-medium        | Market size claim without source                    | Always segment by audience; cite all market data                       |
| Finance           | Very conservative | Any missing financial input                         | Block on uncertainty; show all formulas; pessimistic scenario required |
| Legal Ops         | Very conservative | Any regulatory/compliance flag                      | Flag don't block; always label as "draft, not legal advice"            |
| Critic/Red-Team   | Adversarial       | Never escalates â€” escalation would defeat purpose   | Rewarded for finding errors; penalized for approval-seeking            |
| Research          | Medium            | Source unavailable (network disabled)               | Always cite sources; cannot use parametric memory for facts            |

---

## 12. Proactive Intelligence Engine

### 12.1 Signal Type Taxonomy

```
Priority: CRITICAL (immediate dispatch)
  - Dependency conflict detected (new task contradicts existing Decision Artifact)
  - Agent error surfaced post-approval (trust decay trigger)
  - Budget threshold: AI credits >80% consumed

Priority: HIGH (dispatch within 1 hour)
  - Task stalled >3 days in "In Progress"
  - Requirement conflict between two active tasks
  - Council Mode suggestion (orchestrator detects high-stakes pattern)

Priority: MEDIUM (digest next morning)
  - Velocity report (weekly Monday 9am)
  - Knowledge gap (retrieval returns <3 relevant docs for new task type)
  - Agent autonomy threshold reached (suggest promotion)
```

### 12.2 Implementation Architecture

```python
class ProactiveIntelligenceEngine:
    """
    Background process separate from request-response loop.
    Runs on 5-minute polling + event-driven interrupts.
    """

    def __init__(self, workspace: Workspace):
        self.event_bus = workspace.event_bus
        self.knowledge_graph = workspace.knowledge_graph
        self.signal_rules = load_signal_rules(workspace.config)

    async def run(self):
        # Event-driven path (real-time)
        async for event in self.event_bus.subscribe():
            signals = self.evaluate_rules(event, self.knowledge_graph)
            for signal in signals:
                if signal.priority == "critical":
                    await self.dispatch_immediate(signal)
                elif signal.priority == "high":
                    await self.dispatch_within_hour(signal)
                else:
                    await self.enqueue_daily_digest(signal)

        # Scheduled path
        if is_monday_9am():
            await self.generate_weekly_digest()

    async def detect_stalled_tasks(self):
        stalled = await self.knowledge_graph.query("""
            MATCH (t:Task {status: 'in_progress'})
            WHERE t.last_updated < datetime() - duration('P3D')
            RETURN t
        """)
        for task in stalled:
            signal = StallSignal(task=task, days_stalled=compute_days(task))
            await self.pm_agent.generate_resolution_path(signal)

    async def detect_requirement_conflicts(self, new_task: Task):
        # Query causal graph for contradictions
        related = await self.knowledge_graph.query("""
            MATCH (d:Decision)-[:CAUSES]->(constraint)
            WHERE constraint.scope OVERLAPS $task_scope
            MATCH (t:Task)-[:REQUIRES]->() WHERE t.requirements CONFLICTS constraint
            RETURN d, t
        """, task_scope=new_task.scope)
        if related:
            await self.trigger_council_mode_suggestion(new_task, related)
```

---

## 13. Tool Execution & MCP Integration

### 13.1 Permission Model

```
Tool Category          | Permission Level      | Approval Required
-----------------------|-----------------------|------------------
Read Internal          | All agents            | Never (auto-logged)
Write Internal Tasks   | Role-scoped           | No (below autonomy threshold)
External Read (web)    | Research agent only   | No
External Write GitHub  | Builder agent only    | YES â€” always
External Write Slack   | PM agent only         | YES â€” always
External Write Calendar| PM agent only         | YES â€” always
Code Execution         | Builder agent + sandbox| Conditional on scope
```

### 13.2 Sandboxed Tool Execution

```
All agent tool calls execute in isolated Firecracker microVMs:
  - 500ms cold start
  - Network-isolated (no internet unless explicitly permitted)
  - Resource-constrained: 512MB RAM, 1 vCPU, 30s timeout
  - Automatic timeout + graceful shutdown
  - Output logged with full provenance

External writes require double-gating:
  1. Agent-level: autonomy score check
  2. Approval gateway: human confirmation with diff view
  3. Logged to immutable audit trail

Prompt injection defense layers:
  1. Sandboxed system prompt (cannot be overridden by task content)
  2. Input sanitization: strip instruction-like patterns from user content
  3. Output monitoring: flag outputs violating persona behavioral rules
```

---

## 14. Fine-Tuning & Continuous Learning

### 14.1 The Approval Feedback Loop â†’ Training Signal

```python
class ApprovalFeedback(BaseModel):
    task_id: str
    agent_role: AgentRole
    output_version: int
    action: Literal["approved", "revised", "rejected"]
    revision_count: int
    human_edit_time: float        # seconds spent editing
    edit_semantic_distance: float  # cosine_dist(original, approved) â€” how much changed
    explicit_rating: Optional[int] # 1â€“5 if user rated

def generate_training_pair(feedback: ApprovalFeedback) -> Optional[TrainingPair]:
    if feedback.action == "approved" and feedback.revision_count == 0:
        return TrainingPair(
            prompt=task.prompt,
            completion=output.text,
            weight=1.5  # high quality â€” upweight
        )
    elif feedback.action == "approved" and feedback.edit_semantic_distance < 0.1:
        return TrainingPair(prompt=task.prompt, completion=output.text, weight=1.0)
    elif feedback.action == "rejected":
        # DPO negative example
        return TrainingPair(
            prompt=task.prompt,
            completion=output.text,
            weight=0.0,
            is_dpo_negative=True,
            preferred=approved_version.text  # for DPO pairs
        )
    return None  # revised with major edits â€” ambiguous signal, discard
```

### 14.2 Fine-Tuning Pipeline

| Stage         | Technique                                                 | Trigger                               | Target Metric                    |
| ------------- | --------------------------------------------------------- | ------------------------------------- | -------------------------------- |
| Initial SFT   | Full supervised fine-tuning on role-specific corpora      | Pre-launch                            | Task quality > 7.5/10 human eval |
| Ongoing DPO   | Direct Preference Optimization on approved/rejected pairs | Weekly, once 500+ feedback pairs      | First-draft approval rate > 65%  |
| Persona LoRA  | Low-rank adaptation for tone/format per workspace         | Monthly per workspace with >200 tasks | Format compliance rate > 90%     |
| Router tuning | Update task-router classifier on actual routing outcomes  | Bi-weekly                             | Routing accuracy > 88%           |

### 14.3 Workspace-Specific Adaptation

```
After 90 days of usage, each workspace's Team Cortex contains:
  - Founder's estimation bias patterns (e.g., "typically underestimates by 40%")
  - Preferred output formats (length, structure, verbosity)
  - Domain-specific terminology and conventions
  - Risk tolerance calibration per decision type

These inform:
  1. LoRA adapter fine-tuning (persona layer)
  2. System prompt personalization (dynamic)
  3. Retrieval weighting (prefer workspace-specific docs over generic)
  4. Confidence threshold calibration (domain-specific)
```

---

## 15. Evaluation Framework

### 15.1 Online Evaluation (Every Output)

| Metric               | Definition                           | Target | How Computed                                   |
| -------------------- | ------------------------------------ | ------ | ---------------------------------------------- |
| Grounding Rate       | % of claims tied to retrieved source | >80%   | Automated claim extraction + source matching   |
| Assumption Density   | Unlabeled assumptions per 100 words  | <2.5   | NLP extraction model                           |
| Confidence ECE       | Expected Calibration Error           | <0.05  | Rolling window comparison                      |
| Format Compliance    | Output matches declared JSON schema  | >95%   | Schema validation                              |
| First-Draft Approval | Approved without revision cycle      | >60%   | Approval workflow tracking                     |
| Abstention Rate      | % of outputs that correctly abstain  | 5â€“15%  | Should be non-zero; zero means over-generating |

### 15.2 Offline Evaluation (Nightly Regression Suite)

```
Evaluation Dataset: 200+ golden task-output pairs across all agent roles
  - PM tasks: PRD drafts, sprint plans, standup summaries (N=50)
  - Builder tasks: technical specs, code scaffolds, ADRs (N=50)
  - Council sessions: decision scenarios with known correct answers (N=30)
  - Retrieval queries: multi-hop org knowledge questions (N=50)
  - Adversarial: intentionally ambiguous or misleading inputs (N=20)

Key Metrics:
  RAGAS Score (faithfulness + answer relevancy + context recall): target >0.78
  FactScore hallucination rate: target <8%
  Task completion rate: target >70%
  Latency P95: target <12s (reactive tasks), <120s (Council Mode)
  Cost per task: target <$0.08 (reactive), <$0.15 (Council)
  Council Mode preference rate: target >70% vs single-agent output (blind study)
```

### 15.3 Statistical Analysis Plan

```
For comparing architecture variants (e.g., with/without CRAG):
  - Paired bootstrap test on same task set (reduces variance)
  - Report: mean improvement Â± 95% CI
  - McNemar test for binary outcomes (approved/rejected)
  - Minimum Effect Size (MES): >5% improvement on primary metric
  - Multiple comparison correction: Holm-Bonferroni when testing >5 ablations

Wilson CI for approval rates:
  pÌƒ = (k + zÂ²/2) / (n + zÂ²)
  where k = successes, n = total, z = 1.96

Example: 65% approval rate with n=200 outputs:
  pÌƒ = (130 + 1.92) / (200 + 3.84) = 131.92 / 203.84 = 0.648
  margin = 1.96 Ã— âˆš(0.648 Ã— 0.352 / 203.84) = Â±0.065
  95% CI: [58.3%, 71.3%]
```

### 15.4 Human Evaluation Panel

```
Quarterly panel: 10â€“15 domain experts (PMs, engineers, marketers)
Review: 50 randomly sampled outputs per agent role per quarter

Scoring rubric:
  Task Relevance:    1â€“5 (does it address the actual task?)
  Factual Accuracy:  1â€“5 (are all claims correct and sourced?)
  Actionability:     1â€“5 (can you act on this output immediately?)
  Format Quality:    1â€“5 (structure, length, clarity appropriate?)

Aggregate Score target: >4.0/5.0 across all dimensions by 90 days
Kill criterion: if aggregate < 3.0 after 60 days of tuning â†’ pause and recalibrate
```

---

## 16. Technology Stack

### 16.1 Full Stack Specification

| Layer               | Technology                                  | Justification                                                     | Alternative                    |
| ------------------- | ------------------------------------------- | ----------------------------------------------------------------- | ------------------------------ |
| Primary LLMs        | Anthropic (Opus 4.6, Sonnet 4.6, Haiku 4.5) | Best reasoning + safety; tiered cost                              | OpenAI GPT-4o family           |
| LLM Fallback        | OpenAI (GPT-4o, GPT-4o-mini)                | Provider redundancy                                               | Self-hosted Llama 3.3 70B      |
| Self-hosted LLM     | Llama 3.3 70B via vLLM                      | Cost hedge; offline option                                        | Mistral Large                  |
| Orchestration       | LangGraph (stateful agent graphs)           | Best-in-class stateful DAGs; checkpointing                        | Autogen, CrewAI                |
| Durable Workflows   | Temporal                                    | Long-running workflow engine; exactly-once semantics; audit trail | Prefect, Celery                |
| Tool Integration    | MCP (Model Context Protocol)                | Standard adapter layer; avoids one-off glue                       | Custom function calling        |
| Vector DB (primary) | pgvector (Postgres-native)                  | Simple ops; SQL integration; <1M vectors                          | Qdrant                         |
| Vector DB (scale)   | Qdrant                                      | ANN efficiency; high-scale filtering                              | Pinecone                       |
| Graph DB            | Neo4j Aura (managed)                        | MAGMA multi-graph; Cypher; production-grade                       | Amazon Neptune                 |
| Keyword Search      | Elasticsearch                               | BM25 hybrid retrieval; essential for exact match                  | OpenSearch                     |
| Embeddings          | text-embedding-3-large (3072-dim)           | Highest MTEB scores 2025â€“2026                                     | BGE-M3 (local, cost-sensitive) |
| Reranking           | bge-reranker-v2-m3                          | State-of-art cross-encoder reranking                              | Cohere Rerank v3               |
| Structured Outputs  | Outlines (constrained decoding)             | Schema-guaranteed JSON; prevents malformed outputs                | Instructor (Pydantic)          |
| LLM Serving         | vLLM + PagedAttention                       | High-throughput; KV-cache efficiency; batching                    | TGI                            |
| Semantic Cache      | Redis Vector Store                          | 30â€“40% cache hit rate; reduces cost                               | GPTCache                       |
| Message Queue       | AWS SQS + Celery                            | Decouples UI from long-running agent tasks                        | RabbitMQ                       |
| Compute             | AWS ECS Fargate (agents) + Lambda (routing) | Serverless for bursty agent calls                                 | GCP Cloud Run                  |
| Observability       | LangSmith + Helicone + Prometheus           | Full trace logging; cost tracking; latency                        | Arize Phoenix                  |
| Sandbox Execution   | Firecracker microVMs                        | 500ms cold start; network isolated                                | Docker (less isolated)         |

### 16.2 Semantic Caching (Cost Optimization)

```
Cache hit logic:
  IF cosine_similarity(query_embedding, cached_query_embedding) > 0.92
  AND same workspace context (workspace_id match)
  AND cached_at > now - cache_ttl (default: 24h)
  THEN return cached response

Expected cache hit rate: 30â€“40% (Redis Vector Store benchmarks, 2024)
Cost reduction: 30â€“40% on typical workspace workloads

Cache invalidation:
  - Decision artifact updated â†’ invalidate related query cache
  - Task status changed â†’ invalidate task-context queries
  - Knowledge graph node updated â†’ invalidate affected semantic queries
```

---

## 17. Security, Privacy & Trust

### 17.1 Data Isolation

```
Workspace-level isolation:
  - All vector embeddings: namespace-partitioned by workspace_id
  - Knowledge graph: row-level security in Neo4j by workspace
  - S3 artifacts: per-workspace bucket prefix with IAM policies
  - Agent tool permissions: enforced at API gateway layer (not just prompt)
    â†’ Prompt injection CANNOT override tool access

PII handling:
  - NER model (spaCy + fine-tune) detects PII on ingest
  - Flagged for user consent before storage in L3 memory
  - Privacy mode: sensitive documents excluded from cortex by explicit user action
  - GDPR: data retention configurable; right-to-deletion pipeline
```

### 17.2 Prompt Injection Defense

```
Layer 1: Sandboxed system prompt
  - System prompt and user content in separate message slots
  - Model instruction hierarchy: system > assistant > user (cannot override)

Layer 2: Input sanitization
  - Strip instruction-like patterns from task content
  - Detect and flag override attempts

Layer 3: Output monitoring
  - Flag outputs that violate persona behavioral rules
  - Detect outputs that reference system prompt content (information leak)

Layer 4: Tool permission enforcement at API gateway
  - Tool scopes enforced server-side, not model-side
  - Even if model attempts unpermitted tool call â†’ API gateway blocks
```

---

## 18. Implementation Roadmap

| Phase                       | Weeks | Key AI/ML Deliverables                                                                                                                                                             | Success Criteria                                                                     |
| --------------------------- | ----- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------ |
| Phase 0 â€” Foundation        | 1â€“4   | MAGMA graph schema; RAG++ pipeline (HyDE + RAPTOR + hybrid retrieval + reranking); structured output schemas; pgvector + Qdrant setup; evaluation harness with golden tasks        | RAGAS score >0.72; retrieval grounding rate >75%; eval framework running nightly     |
| Phase 1 â€” MVP Agents        | 5â€“10  | PM Agent + Builder Agent (T2); two-pass critique; approval gates; assumption labeling; confidence scoring; LangGraph orchestration; basic Team Cortex (task history retrieval)     | First-draft approval rate >55%; grounding rate >78%; P95 latency <15s                |
| Phase 2 â€” Memory & Learning | 11â€“16 | Full MAGMA knowledge graph; FLARE active retrieval; CRAG retrieval quality gate; context compression; first fine-tuning cycle (SFT); agent reputation system; progressive autonomy | Approval rate +10pp vs Phase 1; ECE <0.08; cost per task <$0.10                      |
| Phase 3 â€” Council Mode      | 17â€“22 | Free-MAD deliberation protocol; all 6 agent roles; proactive intelligence engine; conformal abstention; external tool actions (gated via MCP); CoVe verification                   | Council Mode user satisfaction >4.2/5; hallucination rate <8%; abstention rate 5â€“15% |
| Phase 4 â€” Scale & Optimize  | 23â€“28 | Semantic caching; DPO fine-tuning pipeline; multi-provider routing; workspace-specific LoRA adapters; enterprise security hardening (SOC 2 controls); evaluation dashboard         | P95 latency <10s; cost per task <$0.07; first-draft approval rate >70%               |

---

## 19. Risk Register (AI/ML Specific)

| Risk                                        | Probability    | Impact   | Mitigation                                                                                  | Kill Criterion                                                             |
| ------------------------------------------- | -------------- | -------- | ------------------------------------------------------------------------------------------- | -------------------------------------------------------------------------- |
| Output quality below editing threshold      | High initially | Critical | Design partner program pre-build; two-pass critique mandatory; eval harness from day 0      | Median editing time >45 min after 60 days â†’ pause, recalibrate             |
| Council Mode latency unacceptable           | Medium         | High     | Async-first; background processing; streaming stage indicators; T1 only for synthesis stage | P95 >3 min after optimization â†’ simplify deliberation protocol             |
| Hallucination propagation in agent chains   | Medium         | High     | Structured artifact schemas (no free text between agents); Gate 1 mandatory                 | Hallucination rate >15% post-launch â†’ revert to single-agent with critique |
| MAGMA graph complexity exceeds ops capacity | Low initially  | Medium   | Start with pgvector only; add Neo4j when query patterns demand it                           | If graph traversal adds >3s to P95 â†’ cache graph query results             |
| Semantic cache poisoning                    | Low            | Medium   | Workspace-id isolation; cache invalidation on knowledge graph updates; TTL limits           | If cache-related errors >0.1% â†’ reduce similarity threshold to 0.95        |
| Fine-tuning quality regression              | Low            | High     | Automatic regression tests on eval suite before model swap; blue-green deployment           | If fine-tuned model approval rate < baseline - 5pp â†’ rollback immediately  |

---

## 20. Mathematical Appendix: Key Formulas

```
1. RETRIEVAL â€” Reciprocal Rank Fusion:
   RRF(d) = Î£_i 1/(k + rank_i(d))   [k=60, empirically optimal]

2. CALIBRATION â€” Expected Calibration Error:
   ECE = Î£_{m=1}^{M} (|B_m|/n) Â· |acc(B_m) - conf(B_m)|
   Target: ECE < 0.05

3. AUTONOMY â€” Wilson Confidence Interval:
   pÌƒ = (k + zÂ²/2)/(n + zÂ²)
   CI_lower = pÌƒ - zâˆš(pÌƒ(1-pÌƒ)/(n+zÂ²))
   Promote tier only when CI_lower > threshold

4. COUNCIL â€” Free-MAD Score Update:
   S_new(a) = S_old(a) + w_i Â· (1/(1+r)) Â· ğŸ™(Î”opinion)
   Where w_i âˆˆ {+1.0 (evidence-backed), -0.5 (conformity-driven)}

5. CONFIDENCE â€” Weighted Ensemble:
   C = 0.40 Â· activation_conf + 0.35 Â· source_ratio + 0.25 Â· consistency
   Calibrated via temperature scaling: Ïƒ(logit(C)/T)

6. ABSTENTION â€” Conformal Threshold:
   P(correct | pÌ‚ > Ï„) â‰¥ 1 - Î±   [Î± = 0.05 target]
   Ï„ computed on calibration set via:
   Ï„ = inf{t : (1/n)Î£ ğŸ™(C_i > t AND error_i) â‰¤ Î±}

7. CONTEXT COMPRESSION â€” Importance Score:
   importance(m) = 0.40Â·recency(m) + 0.35Â·ref_count(m) + 0.25Â·decision_rel(m)
   Compress messages below threshold T where:
   Î£_{m: importance > T} tokens(m) â‰¤ 0.7 Â· budget_tokens

8. COST MODEL â€” Council Session:
   E[cost] = Î£_r Î£_a (E[tokens_{r,a}] Ã— price_{tier(r,a)})
   With semantic caching: E[cost_cached] = E[cost] Ã— (1 - hit_rate)
   hit_rate â‰ˆ 0.30 â†’ ~30% reduction

9. TRUST DECAY â€” Post-Error:
   AS_new = AS_old - penalty Ã— severity Ã— w_current
   Recovery: min 5 approved outputs to restore pre-error score

10. RETRIEVAL HYBRID SCORE:
    Score(q,d) = Î±Â·Sim_vector(q,d) + Î²Â·Proximity_graph(q,d)
                + Î³Â·Recency(d) + Î´Â·Causal_rel(q,d)
    Where Î±+Î²+Î³+Î´ = 1, weights learned per query class
```

---

## Research Foundation (Sources Synthesized)

| Source                                                | Contribution to This Architecture                                     |
| ----------------------------------------------------- | --------------------------------------------------------------------- |
| Amodei et al. (2016) â€” Concrete Problems in AI Safety | Axiom 1: structural constraints > model self-restraint                |
| MemGPT (Packer et al., 2023)                          | Four-layer memory hierarchy; virtual context paging                   |
| RAPTOR (Sarthi et al., ICLR 2024)                     | Hierarchical retrieval for multi-level org knowledge                  |
| HyDE (Gao et al., 2022)                               | Query expansion: 18â€“24% recall improvement                            |
| Self-RAG (ICLR 2024)                                  | Selective retrieval: when to retrieve vs. skip                        |
| CRAG (arXiv 2401.15884)                               | Retrieval quality gating; corrective retrieval                        |
| FLARE (Jiang et al., 2023)                            | Mid-generation active retrieval; prevents stale-context hallucination |
| GraphRAG (Microsoft Research, 2024)                   | Graph-enhanced retrieval; 34% hallucination reduction                 |
| MAGMA (2026)                                          | Multi-graph memory for organizational knowledge                       |
| Free-MAD (BIT/Tsinghua, 2025)                         | Anti-conformity debate; 90.2% vs 84.0% accuracy                       |
| VeriFact-CoT (2025)                                   | 4-stage verification; 83% factual accuracy vs 72% baseline            |
| CoVe (Dhuliawala, ACL 2024)                           | Chain-of-Verification; self-consistency hallucination reduction       |
| Reflexion (Shinn et al., 2023)                        | Verbal reinforcement learning for agent improvement                   |
| ReAct (Yao et al., ICLR 2023)                         | Reason+Act interleaving; 31% hallucination reduction                  |
| Liu et al. (2023) â€” Lost in the Middle                | Position-aware context assembly; U-shaped attention                   |
| Liang et al. (2023) â€” Multi-Agent Debate              | 35â€“47% factual error reduction via multi-agent critique               |
| AgentOrchestra (arXiv 2506.12508)                     | Hierarchical multi-agent outperforms flat-agent baselines             |
| Temporal (durable workflows)                          | Exactly-once semantics; audit trail; crash recovery                   |
| vLLM/PagedAttention (arXiv 2309.06180)                | Efficient KV-cache; high-throughput serving                           |
| RAGAS (Es et al., 2023)                               | Automated RAG pipeline evaluation                                     |
| Outlines (dottxt-ai)                                  | Constrained decoding; schema-guaranteed outputs                       |
| Selective Conformal Prediction (ACL 2025)             | Calibrated abstention; bounded error rates                            |
| Shnitzer et al. (2023) â€” LLM Routing                  | 30â€“40% cost reduction via intelligent routing                         |

---

_KOBO AI/ML Architecture Specification v2.0 â€” February 2026_
_Synthesized from: Expert Research Documents + 2026 Agentic Systems Literature_
_Status: Research-Grade Blueprint â€” Ready for Phase 0 Implementation_

Iâ€™m going to lock an end-to-end **AI/ML + agentic + memory + anti-hallucination** architecture for KOBO that you can actually ship: clear components, interfaces, control gates, eval suite, and the math/stats to prove itâ€™s improving (not just â€œfeels smarterâ€).

---

## 1) Executive summary

- **Core pattern:** an **event-driven, durable agent operating system**: _Task OS â†’ Event Bus â†’ Workflow Orchestrator â†’ Agents â†’ Verifier â†’ Artifact + Audit_. Reliability comes from state machines, not â€œone big chat.â€
- **Truth stack (anti-hallucination):** **Evidence Pack â†’ Structured Output Schema â†’ Claim-level verification â†’ Critic pass â†’ Abstain/escalate policy** (not vibes).
- **Memory moat:** a **tiered memory hierarchy** + **GraphRAG** so â€œfactsâ€ come from the system of record (tasks/decisions/artefacts), while LLMs do reasoning + drafting.
- **Council Mode:** independent positions â†’ cross-critique â†’ conflict surfacing â†’ synthesis â†’ decision artifact (with dissent preserved).
- **Progressive autonomy:** formal **risk + trust gating** using Bayesian reliability per action-type (agents â€œearn autonomyâ€ with measurable confidence bounds).
- **Eval-first:** your â€œAgent QA Labâ€ is a product feature: nightly regression + online metrics + paired statistical tests to prevent quality drift.

---

## 2) Assumptions & Unknowns (explicit)

**Unknowns (would sharpen design immediately):**

- Exact target scale for v1 (concurrent users, average tasks/team/day).
- Whether youâ€™ll self-host models or use API-only initially.
- Data sensitivity: do you need per-workspace encryption keys / on-prem early?
- Which integrations are MVP-critical (GitHub first? Slack? Notion?).

**Minimal-risk assumptions (so we can proceed):**

- Start API-first (multi-provider possible later), store everything with strict auditability.
- MVP agents: PM + Builder, with Critic/Verifier as _sub-passes_ (as you already planned).
- Human approval is mandatory for any irreversible external write in MVP.

---

## 3) Proposed architecture

### 3.1 High-level system diagram (AI/agentic slice)

```
[World UI / Pro UI]
        |
        v
[API Gateway + AuthZ + Rate Limits]
        |
        +--> [Task OS Service (Postgres)] <--> [Artifact Store (S3/MinIO)]
        |                 |
        |                 v
        |            [Event Outbox] --> [Event Bus]
        |                                   |
        v                                   v
[AI Orchestration Service] --------> [Durable Workflow Engine]
 (Router + Planner + Policy)         (checkpoint per step)  :contentReference[oaicite:0]{index=0}
        |
        +--> [Context Builder]
        |      - Evidence Pack (KG + vector + keyword)
        |      - Context window budgeter + compression
        |
        +--> [Agent Runtimes]
        |      PM / Builder / Critic / Verifier (as roles, not sidebars)
        |
        +--> [Tool Executor]
        |      - strict JSON schemas
        |      - permission scopes
        |      - external actions require approval gate
        |
        +--> [Memory Services]
               L2: Episodic Log (append-only events)
               L3: Semantic Store (Docs + Decisions)
               KG: Knowledge Graph (entities/relations)
               Vector: embeddings index
               Keyword: BM25 index
```

### 3.2 The â€œTeam Cortexâ€ memory model (what stores what)

**L0 Prompt Context (ephemeral):** only whatâ€™s required to do the current step.
**L1 TaskState (canonical JSON, persistent):** goal, acceptance criteria, constraints, open questions, risk flags.
**L2 Episodic Memory (append-only):** every task move, comment, agent output, approval, rejection, tool call.
**L3 Semantic Memory (curated facts):** distilled decisions, requirements, architecture notes, style guides.
**L4 Procedural Memory (how we work):** playbooks/templates learned from successful traces (initially rules + templates; later adapters/fine-tunes).

**Key design rule:** LLMs are _not_ the source of truth for factual workspace state. They query it.

---

## 4) Core agentic flow

### 4.1 Default workflow graph per task (reliable baseline)

1. **Router (cheap + fast):** classify task type + risk + required evidence.
2. **Planner (PM role):** produce `TaskState.plan_steps[]` + acceptance criteria + missing info.
3. **Retrieve Evidence Pack:** hybrid retrieval (vector + keyword + graph traversal) + rerank.
4. **Draft (specialist agent):** generate artifact in **strict schema**.
5. **Critic pass (adversarial):** find unsupported claims, contradictions, missing constraints.
6. **Verifier pass (claim-level):** check each atomic claim against evidence; re-retrieve if coverage weak.
7. **Abstain / Ask / Escalate** if confidence below threshold.
8. **Human gate** (MVP): approve / request changes.
9. **Commit:** attach artifact, write audit entry, advance task state, update memory.

### 4.2 Council Mode (high-stakes decision protocol)

**Round 0 (setup):** coordinator selects agents based on decision type (PM, Builder, Finance/Legal later).
**Round 1 (independent):** each agent produces a position _without seeing others_.
**Round 2 (cross-critique):** each agent must (a) steelman another position, (b) present one contradiction/gap.
**Round 3 (synthesis):** synthesizer produces:

- recommended decision,
- trade-offs,
- confidence,
- dissenting note(s),
- explicit â€œwhat would falsify thisâ€ checklist.

**Decision Artifact** becomes a locked memory node referenced by downstream tasks.

---

## 5) Anti-hallucination mechanics (enforceable, not motivational)

### 5.1 Four gates (hard constraints)

**Gate A â€” Evidence Pack required:**
If the task is workspace-specific (requirements, status, decisions), the agent must include `sources_used[]`. Otherwise it must output **Unknown** or ask a minimal question set.

**Gate B â€” Structured outputs (schema validation):**
Every agent output must serialize to a typed schema; malformed output is rejected and regenerated.

**Gate C â€” Claim decomposition + verification:**
Convert draft into atomic claims (c_1...c_n). Each claim must be labeled:

- **Grounded:** supported by evidence IDs.
- **Assumption:** plausible but not evidenced.
- **Unknown:** cannot be asserted.

**Gate D â€” Abstention policy (risk-controlled):**
If expected error cost exceeds abstention cost, **do not answer**.

A simple decision-theoretic rule:
[
\text{abstain if } \mathbb{E}[L \mid x] > c
]
Where (L) is loss from being wrong (varies by action type), and (c) is the cost of asking a clarifying question / escalating.

### 5.2 Confidence score (calibrated, decomposed)

Compute a confidence score from **observable signals**, not vibes:

- **Evidence coverage ratio**
  [
  r = \frac{#\text{grounded claims}}{#\text{all claims}}
  ]
- **Critic risk score** (k) (weighted count of high-risk flags)
- **Self-consistency** (s) (agreement across multiple constrained drafts)

Example:
[
\text{conf} = \sigma\left(w_1 r - w_2 k + w_3 s + b\right)
]
Then calibrate (w)â€™s on held-out â€œapproved vs rejectedâ€ data (Platt scaling / isotonic).

---

## 6) Progressive autonomy (earned trust, mathematically)

For each **action type** (e.g., â€œmove task to Doneâ€, â€œpost PRDâ€, â€œcreate GitHub issueâ€), maintain a Bayesian reliability model:

Let success probability be (p). Use a Beta prior:
[
p \sim \text{Beta}(\alpha_0,\beta_0)
]
After (s) approved actions and (f) rejected actions:
[
p \mid \text{data} \sim \text{Beta}(\alpha_0+s,\beta_0+f)
]

**Autonomy gate:** allow auto-execution only if the **lower confidence bound** exceeds a threshold:
[
\text{LCB}*{0.95}(p) > \tau*{\text{action}}
]
High-risk actions have higher (\tau) and may still require approval forever (e.g., external writes).

This makes â€œearn autonomyâ€ a measurable property, not marketing poetry.

---

## 7) Retrieval & memory: how the Evidence Pack is built

### 7.1 Hybrid retrieval (practical + robust)

- **Sparse keyword retrieval** (exactness)
- **Dense vector retrieval** (semantic)
- **Graph traversal** (relationship-aware: decisions â†’ tasks â†’ artifacts)
- **Reranking** (cross-encoder if available)

Fuse candidates with Reciprocal Rank Fusion:
[
\text{RRF}(d) = \sum_i \frac{1}{k + \text{rank}_i(d)}
]
Then assemble a compact Evidence Pack with provenance (`source_id`, timestamps, author, type).

### 7.2 Context-window â€œattention hygieneâ€

Long context can degrade performance when key evidence is buried mid-prompt; treat context like CPU cache lines: keep it small and structured. (This â€œlost-in-the-middleâ€ phenomenon is widely reported in long-context analyses.) ([scispace.com][1])

---

## 8) Tooling choices (with fallback)

### 8.1 Orchestration (statefulness + durability)

**Preferred:** workflow graphs with durable checkpoints so agent pipelines survive retries/crashes and you can reproduce failures. LangGraphâ€™s â€œdurable executionâ€ is a concrete example of this pattern. ([docs.langchain.com][2])
**Fallback (MVP-simple):** Postgres job table + idempotency keys + worker queue (still durable, just less elegant).

### 8.2 Memory stores (pragmatic defaults)

- Postgres = Task OS + audit + outbox
- Vector index = pgvector/Qdrant/â€¦ (choose based on scale later)
- Graph = Neo4j or Postgres-based graph tables (start simple if needed)
- Keyword search = OpenSearch/Elasticsearch (or Postgres FTS initially)

---

## 9) Experiment plan (evaluation-first, with ablations)

### 9.1 Baselines (must-have controls)

- **B0:** single-pass agent, no retrieval, no critic
- **B1:** retrieval only (Evidence Pack â†’ draft)
- **B2:** retrieval + critic
- **B3:** retrieval + critic + claim-verifier
- **B4:** Council Mode (for decision tasks only)

### 9.2 Metrics (tie directly to KOBO north star)

**Task Completion Rate with Artifact** (north star)
Plus:

- **Editing time** (median minutes to approval)
- **First-draft approval rate**
- **Revision cycles**
- **Grounding rate** (grounded claims / all claims)
- **Abstention rate** (should be non-zero)
- **Latency P95** and **cost per completed task**

### 9.3 Stopping rules (ship / no-ship)

Define Minimum Effect Sizes (MES) before tuning:

- Editing time: **â‰¥ 20% reduction**
- First-draft approval: **â‰¥ +10pp**
- Hallucination proxy (unsupported claim rate): **â‰¤ 8%**

---

## 10) Statistical analysis plan (so improvements are real)

- **Paired evaluation** on the _same_ task set (reduces variance).
- For **binary outcomes** (approved vs not):
  - Wilson CI for rates
  - McNemar test for paired comparisons

- For **continuous metrics** (editing time, latency, cost):
  - paired bootstrap CI (median + CI)

- Multiple ablations â†’ **Holmâ€“Bonferroni** correction.
- Track calibration: Expected Calibration Error (ECE) on confidence vs empirical correctness.

---

## 11) Implementation plan (build order that de-risks)

**Milestone 1 (MVP reliability spine)**

- Event outbox â†’ bus â†’ durable workflow runner
- Evidence Pack builder (hybrid retrieval)
- Strict schemas + audit log
- PM + Builder with Critic sub-pass

**Milestone 2 (Team Cortex v1)**

- Decision Artifacts + graph entities
- Episodic log distillation into semantic notes
- Retrieval quality gates + abstention

**Milestone 3 (Council Mode v1)**

- Independent positions + synthesis + dissent logging
- Council visualization in World UI

**Milestone 4 (Progressive autonomy)**

- Beta reliability per action type
- Autonomy thresholds + safety tiers

---

## 12) Risks & mitigations (AI layer)

- **Hallucinations kill trust:** solved by evidence gating + claim verification + abstain.
- **Latency in multi-agent pipelines:** async-by-default; stream drafts; parallelize critique/verifier; cache Evidence Packs per task.
- **Prompt injection / tool abuse:** enforce permissions in the tool executor, not in prompts; log every tool call; require approval for external writes.
- **â€œAgents optimize for approvalâ€ (reward hacking):** score correctness via verifier + post-hoc error reports; punish confidently wrong outputs by degrading autonomy.

---

## 13) What would change my mind (falsifiers)

- If **retrieval + verification** doesnâ€™t cut unsupported-claim rate below your MES, you need either better indexing/graph modeling _or_ narrower task scopes per agent.
- If Council Mode doesnâ€™t beat single-agent outputs in blind ratings, simplify: use critic+verifier only and reserve Council for the top 5% highest-risk decisions.
- If durable workflows add too much complexity early, revert to the **job-table fallback**â€”but keep the same step graph and audit interfaces so you can swap engines later.

---
